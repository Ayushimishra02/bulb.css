Sign Language Image Classification

This project uses Convolutional Neural Networks (CNNs) to classify hand gestures representing sign language alphabets using image data. It was developed and tested in Google Colab using TensorFlow and Keras.

1)Project Description

The model is trained on a dataset of hand gestures representing American Sign Language (ASL) alphabets. It includes:
- Image preprocessing and augmentation
- CNN-based model training
- Accuracy and loss visualization
- Model evaluation on test data

2)Dataset

The dataset was downloaded from Kaggle:  
[ASL(american sign language) Alphabet Dataset](https://www.kaggle.com/datasets/ayuraj/asl-dataset)

You can either upload `kaggle.json` in Colab or manually download the dataset from the Kaggle link above.

Technologies Used

- Python
- Google Colab
- TensorFlow / Keras
- NumPy, Matplotlib
- Kaggle API

3)How to Run

1. Clone the repo or open the `.ipynb` in Google Colab.
2. Upload your `kaggle.json` credentials to access the dataset.
3. Run the cells in order to:
   - Download and extract dataset
   - Preprocess and split images
   - Train the CNN model
   - Evaluate accuracy and visualize results

#you can directly run this code in Google Colab from scratch.It contains all necessary files

5)Results

- Achieved high training and validation accuracy
- Visualized training loss vs validation loss
- Prediction sample visualizations

6)Author

**Ayushi Mishra**  
Connect with me on [LinkedIn(https://www.linkedin.com/in/ayushi-mishra-495651350/)]
7) Credits

Thanks to [Kaggle](https://www.kaggle.com/) for the dataset and Colab for the free GPU support.


THANK YOU

