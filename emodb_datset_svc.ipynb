{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQOpQvmnCch93dy3fVbIfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushimishra02/bulb.css/blob/main/emodb_datset_svc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEl22ae_ZBMQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "PX5GHvEZZNTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d piyushagni5/berlin-database-of-emotional-speech-emodb\n",
        "\n"
      ],
      "metadata": {
        "id": "ZcFsgVuHZxTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o berlin-database-of-emotional-speech-emodb.zip -d emodb_data"
      ],
      "metadata": {
        "id": "HdxUKIBsaKLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os #for making path for file folder in this case the speech files\n",
        "import librosa #for conversion of speech files into melspectograms\n",
        "import librosa.display #to display as melspectograms to understand it\n",
        "import matplotlib.pyplot as plt\n",
        "data_dir=\"emodb_data\"\n",
        "emotion_map={\n",
        "    'W':'Angry','L':'Bored','E':'Disgust',\n",
        "    'A':'Fearful','F':'Happy','T':'Sad','N':'Neutral'\n",
        "}"
      ],
      "metadata": {
        "id": "swbVKayhcAqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(file_path): #we extract features from the file path so we can use it for every audio fiiles\n",
        "  y,sr=librosa.load(file_path,sr=160000) #y is the number that represent how loud is the sound at each momemt and 16000 is the no.of samples persecond which is voice resolution\n",
        "  spect=librosa.feature.melspectrogram(y=y,sr=sr,n_mels=128) #mel_spects 128 is how detailed the image would be would be and melspectogram means making a clear and good quality picture of graph of speech noices\n",
        "  #mel means the pitch recorded and heard like a human not robot\n",
        "  #converting the picture from raw intensity to decibel scale means making the high pitch places brighter and less noise places darker\n",
        "  spect_db=librosa.power_to_db(spect,ref=np.max)\n",
        "  return spect_db"
      ],
      "metadata": {
        "id": "0Xrp0PENr1Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#librosa is python library for music and audio analytics"
      ],
      "metadata": {
        "id": "f1bAulQz6sds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os #helps in providing files or folders path in ur computer\n",
        "import numpy as np\n",
        "X=[] #x has feature in vector fomr\n",
        "Y=[] #y has emotion label of angry or happy etc.\n",
        "for root, _, files in os.walk(data_dir): #os.walk helps in iterating through each folder and its subfolder and its next part inside sub folder so in one line you get all ur audio files and u do not need to manually import all of them here\n",
        "    for file in files:#_use the files which we do not use files here thorway files\n",
        "      if file.endswith('.wav'): #.wav is the audio classification of all the datas inside the datset it makes sure that no other format datas get here\n",
        "        file_path=os.path.join(root,file) #os.path.join makes sure that every .wav file path is used here it give the exact location of every .wav file in them and also helps in running it in every possible os like windows,linux,macos\n",
        "        emotion_code = file[5] #5 means use the 6th character in the file name that is its actual emotion calling like \"03010L01\"\n",
        "        if emotion_code in emotion_map: #and whatever that 5th code is it checks if it is in the emotion map\n",
        "            emotion = emotion_map[emotion_code] #if its there add i to emotion tab what its actual value is not key\n",
        "            #print(f\"File: {file}, Emotion: {emotion}\") #then print it\n",
        "            spect_db = extract_features(file_path)#it helps in extracting all features of graphs in file_path\n",
        "            #librosa.display.specshow(spect_db, sr=160000, x_axis='time', y_axis='mel')#librosa.display.specshow makes the spectograms and 16000 iis no.of samples taken at a time\n",
        "            #plt.colorbar(format='%+2.0f dB')#colobar helps in providing a colorbar at the side of the graph showing darkis equal to loud sound and soft colors is normal low sounds\n",
        "            #plt.title(f'Mel-spectrogram for {emotion}')\n",
        "            #plt.show() #plt is like plotting a graph based on given instructions\n",
        "            feature_vector=np.mean(spect_db,axis=1) #we are making a 2d file into 1d to flatten it so axis of time is only here becuase x was time axis\n",
        "            X.append(feature_vector)\n",
        "            Y.append(emotion_map[emotion_code])"
      ],
      "metadata": {
        "id": "Tl3P5-hIxyA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "X=np.array(X)\n",
        "Y=np.array(Y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.22) #random state 42 ensures you get the same split everytime u run it\n",
        "clf=SVC(kernel='linear',C=1) #linear uses st. line to sperate class and c=1 balances accuracy and simplicity\n",
        "clf.fit(X_train,y_train) #model learns from pattern x_train to predict on y_train\n",
        "y_pred=clf.predict(X_test)\n",
        "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
        "print(\"Classification_report\",classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "xoueLwlIY5pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in output it is precision meaning out of so many sounds how precise u were to classify if the sound is happy ,sad\n",
        "#recall is how much of the actual particular tracks(happy,sad..), model was able to vatch coorectly\n",
        "#f1 is basically calculating mean value for the precision and recall"
      ],
      "metadata": {
        "id": "sNXGM3Pslmfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "metadata": {
        "id": "7apwl3mafDHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "file_name=list(uploaded.keys())[0] #i want my predicting model to give svm the name of the file we are uploading without the name svm wont know to work with wht\n",
        "features=extract_features(file_name)\n",
        "feature_vector=np.mean(features,axis=1)\n",
        "feature_vector=feature_vector.reshape(1,-1)\n",
        "predicted_emotion_code=clf.predict(feature_vector)[0]\n",
        "reverse_map={v:k for k,v in emotion_map.items()}\n",
        "predicted_emotion=reverse_map.get(predicted_emotion_code,\"unknown\")\n",
        "print(f\"predicted emotion:{predicted_emotion_code},predicted emotion code:{predicted_emotion}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eS6d2TeKeMxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}